---
title: "Seth Hall - Midterm 1"
author: "Seth Hall"
date: "2024-09-23"
output: pdf_document
---


Adressing Bias in Healthcare Algorithms: A Fairness Analysis of Predictive Models


Introduction

       Prediction algorithms are becoming more and more prevalent and important in many different areas of healthcare 
such as decision-making, prioritizing patients, allocating resources, and even diagnosing conditions. However, as they 
do become more and more prevalent, more concerns are being raised about their fairness and whether they reflect or amplify 
existing societal biases. Many stidues have shown that algorithms can unintentionally lead to unequal access to care. This 
especially has negative consequences for marginalized groups. This includes underrepresented racial demographics and 
low-income patients. These types of decisions have life-altering implications, so addressing such biases is particularly 
urgent.
      
       A study titled "Dissecting racial bias in an algorithm used to manage the health of populations" by Obermeyer et. al. 
that unearthed significant racial disparities within a widely used predictive model, the name of which wasn't mentioned. The 
model in question used medical spending data from the past as a proxy for health needs, and the study by Obermeyer et. al. 
found that this decision led to the underestimation of health needs for black patients compared to white patients. Through 
statistical measures of fairness, Obermeyer et al. demonstrated the model’s systematic bias and provided quantitative proof 
of how these algorithms could reinforce healthcare inequities. 
       
Summary of Method

       The model that was analyzed in the study was intended to predict patient healthcare needs based on their historical 
healthcare spending. The objective of Obermeyer et. al. was to determine whether fairly represented patients from different 
racial groups. Specifically, the study focused on whether the model was systematically undervaluing black patients’ health 
risks because they historically spent less on health care, which is a factor that is influenced by socioeconomic barriers to 
accessing care. 
       
       The study employed a large and diverse data set from U.S. hospital systems. This data set contained health metrics, 
demographic information, and prior healthcare spending data for patients. The authors used this data set to analyze how the 
algorithm distributed risk scores and whether these scores adequately reflected patients’ actual health needs across racial 
groups. Healthcare spending was used as a primary variable, and because of this it favored people who had a history of 
higher spending on healthcare, which tended to correlate with white patients who have historically had better access to 
healthcare than black patients. 
       
       To evaluate fairness within this model, the researchers implemented several key measures of fairness that have been 
discussed in STOR 390, including statistical parity, equalized odds, and disparate impact. These measures of fairness 
allowed the study to quantify the bias in the algorithm’s predictions. This made the disparities not only visible, but 
also mathematically demonstrable.
       
       The first fairness measure applied was statistical parity. Statistical pairity is a method that adjusts data so that 
decisions are made fairly without discrimination, with the goal of ensuring the same probability of inclusion in the 
positive predicted class for each sensitive group. In this case, the sensitive groups were patients that were identified as 
"high-risk." In the context of this study, statistical parity would imply that black and white patients with similar medical 
conditions should have an equal probability of being flagged for high-priority healthcare. The researchers found that the 
model didn't meet statistical parity because black patients were less likely to be assigned high-risk scores compared to 
white patients, even if they had similar health profiles. The model clearly demonstrated imbalance because patients from 
different racial groups received unequal outcomes based on historical spending rather than actual health needs. This was a 
crucial first step in highlighting the model’s inequitable outcomes.
       
       The next measure of fairness that the researchers applied was an equalized odds test. Equalized odds tests examine 
the error rates of a model across different groups, specifically comparing false positive and false negative rates. 
Equalized odds is especially important in healthcare because an error in classification can lead to misallocation of care. 
For example, a false positive where a patient that doesn't have a condition tests positive for it may cause unnecessary 
allocation of medicine that could cause the patient harm while also taking it away from another patient who may have 
actually needed it. Obermeyer et al. found a statistically significant disparity in the false negative rates between black 
and white patients. Black patients were often categorized as low-risk, even when their health needs were comparable to 
those of white patients who received high-risk scores. This unequal error distribution highlighted the model’s tendency to 
under-serve black patients. Because of this, the model failed to meet the equalized odds fairness measure.  
       
       The third measure of fairness the researchers used was a disparate impact analysis. Disparate impact is a practice 
that negatively affects a protected group of people more than another, even though the rules appear neutral. Disparate 
impact is determined by calculating the ratio of favorable outcomes across demographic groups and assessing whether this 
ratio remains within a specified threshold. In their analysis, Obermeyer et al. observed a marked disparate impact against 
black patients. Black patients were less likely to be identified as high-risk than white patients with similar medical needs. 
By using relative risk ratios, the researchers showed that black patients’ risk scores were 48% lower than those of White 
patients with similar health profiles on average. Black patients with comparable needs to white patients were consistently 
deprioritized because they historically spent less on healthcare. This highlights a clear bias in how the model assigned 
risk scores. 
       
Results
       
       Through statistical analysis and measures of fairness, the researchers could determine that black patients were 
systematically assigned lower health risk scores. The difference was not only statistically significant but reproducible. 
This is an important aspect of the study's reproducibility. Using subgroup analysis and effect size calculations, 
Obermeyer et. al. demonstrated the algorithm’s impact with a lot of clarity. Unlike the black boxed COMPAS algorithm that 
has been discussed in STOR 390, Obermeyer et. al. made their methods, data, and sources accessible, allowing other 
researchers to verify or add onto their findings. The study’s reproducibility is important because it enables the healthcare 
community to examine the bias within this predictive model and potentially others. 
       
Normative Concern
       
       The normative implications of this study are profound. Algorithmic bias in healthcare extends beyond statistics to 
ethical issues with extreme real-world consequences. Not only do biased algorithms reflect existing dispairities between 
groups, but they cause them to grow over time. From an ethical standpoint, fairness in healthcare algorithms used for 
prediction is essential to prevent the continuation of historical biases. In the model studied by Obermeyer et. al., the 
using healthcare spending as a proxy for need negatively affected patients from underserved communities. And, due to 
structural inequities, had limited access to healthcare and lower spending histories. This choice of proxy introduced an 
indirect form of discrimination, by which a neutral factor like spending became a determinant of care.
       
       The implications are clear that healthcare providers and policymakers need to carefully evaluate the variables used 
in predictive models like this one. It's important so that they genuinely reflect health needs instead of reinforce 
socioeconomic biases. This study highlights the moral obligation to include fairness into healthcare algorithms, and it
calls for more accountability in how these tools are made and used. 
       
Future Directions
       
       The findings from Obermeyer et. al. highlight the need for policy measures to regulate healthcare algorithms, 
particularly concerning fairness. One potential policy solution could be mandatory audits to ensure fairness in predictive 
models used in healthcae. These audits would ensure that models are rigorously tested for disparate impact, equalized odds, 
and other fairness metrics before being used in healthcare. Moreover, policies like this could help both healthcare providers 
and patients understand the basis of risk assessments and how to prioritize who gets treated. Requiring algorithms to be 
explainable would make it easier to identify potential sources of bias and eliminate them before they continue. Additionally, 
funding research into alternative fairness metrics and proxy variables could provide healthcare systems with new tools to 
ensure that prediction models reflect true health needs rather than indirect socioeconomic factors.
       
Conclusion
       
       All in all, Obermeyer and contributors' study provides a powerful demonstration of the role that the measures of 
fairness discussed in STOR 390 can play in identifying and quantifying biases when it comes to healthcare algorithms. By 
applying statistical parity, equalized odds, and disparate impact, the researchers illustrated how a common predictive model 
under-served black patients by relying on a cost-based proxy for health need. Their findings offer a quantitative, 
reproducible account of the model’s bias. This highlights the necessity of implementing fairness into healthcare algorithms 
to prevent the perpetuation of health disparities.
       
       The normative and policy implications of this study are far-reaching. As predictive algorithms continue to influence 
healthcare delivery, ensuring their fairness and accountability is becoming a larger issue in society. Policy makers, 
healthcare providers, and the developers of these algorithms all must recognize that fairness is not merely a technical 
consideration; it is a foundational principle that upholds equity in healthcare. By advancing policies that promote 
fairness, a healthcare system can be developed where algorithms serve all patients equitably, regardless of their 
background or socioeconomic status.
  
  
  Obermyer, Z., Powers, B., Mullainathan, S. "Dissecting racial bias in an algorithm used to manage the health of 
populations." Science, Vol 366, Issue 6464. 25 October 2019. https://www.science.org/doi/10.1126/science.aax2342
